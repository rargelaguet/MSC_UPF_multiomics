---
title: "Principal Component Analysis"
author: "Ricard Argelaguet"
date: "`r Sys.Date()`"
---

- Plot loadings
- Interpret factor and loading values
- What happens if we scale the value of height by XX?
- 



```{r}
# M <- matrix(c(1,1,1,1e-10,0,0,0,1e-10,0,0,0,1e-10), nrow=4, ncol=3, byrow = T)
# eigen(cov(M))
# svd(cov(M))
```

```{r, echo = FALSE, message = FALSE}
# knitr::opts_chunk$set(collapse = TRUE,comment = "#",fig.width = 5,
#                       fig.height = 4,fig.align = "center",
#                       fig.cap = "&nbsp;",results = "hold")
```

```{r, message = FALSE}
library(ggplot2)
library(tidyverse)
library(ggpubr)
```

# Load the data

```{r}
data(iris)
head(iris)
```

```{r}
Y <- iris[-5] %>% as.matrix %>% scale(center=TRUE, scale=FALSE)
N <- nrow(Y)
D <- ncol(Y)
```

# Compute PCA using the eigen decomposition

Calculate covariance matrix
```{r}
S <- (t(Y)%*%Y) / (N-1)
```

Calculate eigendecomposition (rank 2) of the covariance matrix. If you forgot how eigenvectors are calculated you can check [this page](https://rpubs.com/aaronsc32/eigenvalues-eigenvectors-r)
```{r}
eigenvectors <- eigen(S)$vectors[,c(1,2)]  # Corresponds to the feature loadings
dim(eigenvectors) 
```


Eigenvalues correspond to the standard deviation of the principal components
```{r}
eigenvalues <- eigen(S)$values[c(1,2)]
head(sqrt(eigenvalues))
```

Multipling the original data by the eigenvectors (also called the rotation matrix) projects the data to make the principal components the new axes
```{r}
PCs <- (Y %*% eigenvectors)
head(PCs)
```

The interpretation of the PCs values is not trivial. Each PCA corresponds to an orthogonal axis of variation. Each axis is made up by  a linear combination of the input features, where the linear coeficients are given by the feature weights. Importantly, the feature means (the intercepts) have been substracted from the data. Hence, each PC ordinates the samples along a one-dimensional axis centered at zero. Samples with different signs manifest opposite "effects" along the inferred axis of variation, with higher absolute value indicating a stronger effect.
```{r}
df <- as.data.frame(PCs) %>% 
  cbind(as.character(iris$Species)) %>%
  `colnames<-`(c("PC1","PC2","species"))

ggscatter(df, x="PC1", y="PC2", fill="species", shape=21 ,size=3) +
  labs(x="Principal component 1", y="Principal component 2") +
  theme(
    legend.position = "right",
    axis.text = element_text(size=rel(0.8))
  )
```

# Compute PCA automatically using `prcomp` 

```{r}
prcomp.pca <- prcomp(iris[1:4], rank.=2)
names(prcomp.pca)
```

```{r}
# "rotation" corresponds to the "weight/loading matrix" (the matrix whose columns contain the eigenvectors)
dim(prcomp.pca$rotation)

# "x" corresponds to the principal components (the data multiplied by the rotation matrix)
dim(prcomp.pca$x)

# "sdev" contains the standard deviation for each PC
prcomp.pca$sdev
```

Plot the PCs
```{r}
df <- as.data.frame(prcomp.pca$x) %>% 
  cbind(iris$Species) %>%
  `colnames<-`(c("PC1","PC2","species"))

ggscatter(df, x="PC1", y="PC2", fill="species", shape=21 ,size=3) +
  labs(x="Principal component 1", y="Principal component 2") +
  theme(
    legend.position = "right",
    axis.text = element_text(size=rel(0.8))
  )
```


# Exploration of the feature weights/loadings

The feature weights/loadings provide a score for how strong each feature relates to each factor. Features with no (linear) association with the PC have values close to zero whereas features with strong association with the factor have large absolute values. The sign of the weight indicates the direction of the effect: a positive weight indicates that the feature has higher levels in the samples with positive factor values, and vice versa.

```{r}
to.plot <- pca.original$rotation %>%
  as.data.frame %>% tibble::rownames_to_column("feature") %>%
  pivot_longer(cols=starts_with("PC"), names_to = "PC", values_to = "value")

ggbarplot(to.plot, x="feature", y="value", facet="PC", fill="feature", position = position_dodge(0.9)) +
  labs(x="", y="Loading") +
  theme(
    legend.position = "right",
    legend.title = element_blank(),
    axis.text.x = element_blank()
  )
```

Sepal.Length has a positive loading for PC1. This means that the Sepal Length increases with increasing PC1 values (from negative to positive). In contrast, it has a loading of almost zero for PC4, which suggests that it has no influence on this source of variation. Let's check this:
```{r}
to.plot <- as.data.frame(pca.original$x) %>%
  mutate(sepal_length=iris$Sepal.Length) %>%
  pivot_longer(cols=starts_with("PC"), names_to = "PC", values_to = "pc_value")
  
ggscatter(to.plot, x="pc_value", y="sepal_length", facet="PC", scales="free",
  add="reg.line", add.params = list(color="blue", fill="lightgray"), conf.int=TRUE) +
  stat_cor(method = "pearson") +
  labs(x="PC value", y="Sepal length") +
  theme(
    axis.text = element_text(size=rel(0.8))
  )
```

# Questions

## (Q) What happens to the Principal Components (the sample scores) if you add a constant term to one of the variables? and what if you scale one of the variables by 100x? and to the feature loadings? Should we scale the data before PCA?

(ANSWER) Differences in mean (i.e. constant intercept) make no difference in PCA, as the method only exploits the patterns of covariation. Remember that the input to PCA is the covariance matrix, where the effect of means have been removed. This is why the data is always centered when runnng PCA.

Scaling does make a difference in PCA. Features with larger variance dominate the covariance matrix and hence tend to contribute more to the principal components. Data should be scaled when the differences in variance between the features are not considered biologically meaningful. In this case, if width and length were measured in different units then standarisation to unit variance would be a good idea.

```{r}
Y <- iris[-5] %>% as.matrix

# Add constant
Y.scaled[,1] <- 10*Y.scaled[,1]

# Sxale
Y.scaled[,1] <- Y.scaled[,1] + 1
```

```{r}
colMeans(Y)
colMeans(Y.scaled)
```

```{r}
apply(Y,2,sd)
apply(Y.scaled,2,sd)
```

```{r}
pca.original <- prcomp(Y)
pca.scaled <- prcomp(Y.scaled)
```

```{r}
df.original <- pca.original$x %>% 
  as.data.frame %>%
  mutate(sample=as.character(1:N)) %>%
  mutate(type="original") %>%
  pivot_longer(cols=starts_with("PC"), names_to = "PC", values_to = "value")

df.scaled <- pca.scaled$x %>% 
  as.data.frame %>%
  mutate(sample=as.character(1:N)) %>%
  mutate(type="scaled") %>%
  pivot_longer(cols=starts_with("PC"), names_to = "PC", values_to = "value")

df <- rbind(df.original, df.scaled) %>%
  pivot_wider(id_cols=c("sample","PC"), names_from="type", values_from="value")
  

ggscatter(df, x="original", y="scaled", facet="PC") +
  labs(x="Original value", y="Scaled value") +
  geom_abline(slope=1, intercept=0) +
  theme(
    legend.position = "right",
    axis.text = element_text(size=rel(0.8))
  )
```

<!-- Many machine learning models and algorithms require data to be standarized, usually by centering the features to zero-mean and optionally scaling them to unit variance. Whether and how to standarize your data is not a straight-forward answer and it depends on what your aim is, on what method you are using and what your data looks like. -->

<!-- As a general rule, PCA tries to capture as much variability as possible in the data, so the features with high variability will explain more fraction of the total variance than features with low variability. In some ocasions this is what we want, for example, if we have gene expression data we want the more variable genes (usually the differentialy expressed genes) to contribute more than the genes which are expressed in constant levels (housekeeping genes). -->

<!-- However, there are cases where some features have more variance because of technical reasons, for example if the scale of the variables is different (centimeters vs meters). In this case, we need to standarise the data to unit variance, otherwise the features in centimetres will have higher scale and will contribute more to the total variance. Regarding feature centering, PCA is invariant to linear transformations of the data, so adding constant terms to the variables do not affect the output. However, it is common practice and always recommended to center the data. -->


