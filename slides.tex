\documentclass[aspectratio=169,notes]{beamer}

\usepackage{pgfpages}

% Allows including images
\usepackage{graphicx} 

% ???
\usepackage[english]{babel}

%\setbeameroption{show notes on second screen}
%\setbeameroption{hide notes}

% Use metropolis theme
\usetheme[
	outer/progressbar=foot,
	outer/numbering=counter,
	%titleformat=allsmallcaps
	]{metropolis} 
\definecolor{blue}{HTML}{056466}
\definecolor{white}{HTML}{FFFFFF}

 % Sans Serif Font
%\usepackage[semibold,oldstyle]{sourcesanspro}

 % The command \nth{<number>} generates English ordinal numbers of the form 1st, 2nd, 3rd, 4th
%\usepackage[super]{nth}

% Theme colors are derived from these two elements
\setbeamercolor{alerted text}{fg=blue}
\setbeamercolor{frametitle}{bg=blue}
\setbeamercolor{normal text}{bg=white}

% Set EBI logo footer
%  \setbeamertemplate{footline}{%
%         \includegraphics[height=1cm]{img/EBI_logo.png}%
% 		%\hfill%
% 		% \insertsectionnavigationhorizontal{.5\textwidth}{}{}
% }


\title{Statistical methods for data integration}
\author{Ricard Argelaguet \\ \textit{ricard@ebi.ac.uk}}
\institute{European Bioinformatics Institute (EMBL-EBI) \\ University of Cambridge}
%\date{\today}


%  (SI) units 
\usepackage{siunitx}

% bibliography
% \usepackage[style=authoryear, backend=biber, sortlocale=en_US, natbib=false, url=false, doi=true, eprint=false]{biblatex}
\usepackage[style=authoryear,,backend=bibtex]{biblatex}
\addbibresource{references/bibliography.bib}

% \renewbibmacro*{cite}{
%   \iffieldundef{shorthand}
%     {\ifthenelse{\ifnameundef{labelname}\OR\iffieldundef{labelyear}}
%        {\usebibmacro{cite:label}
%         \setunit{\printdelim{nonameyeardelim}}}
%        {\printnames{labelname}
%         \setunit{\printdelim{nameyeardelim}}}
%      \usebibmacro{cite:labeldate+extradate}
%      \setunit{\addcomma\space}
%      \usebibmacro{journal}}
%     {\usebibmacro{cite:shorthand}}
% }

\DeclareCiteCommand{\footcite}[\mkbibfootnote]
  {\usebibmacro{prenote}}
  {\printnames{author}
   \printfield{title}{}
   \printfield{journaltitle}{}
   \printfield{year}}
  {\addsemicolon\space}
 {\usebibmacro{postnote}}
  \newcommand\footnotenonum[1]{
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}
  \addtocounter{footnote}{-1}
  \endgroup
}

\input{utils.tex}


\begin{document}

 	% Print the title page as the first slide
	\begin{frame}
	\titlepage
	\end{frame}
	
	% \begin{frame}{Overview}
	% \end{frame}

	\begin{frame}{Why multi-omics?}
	\centering
	\includegraphics[height=6cm]{img/Ritchie2015_overview.png}\footcite{Ritchie2015}
	\end{frame}
	
	\begin{frame}{Why multi-omics?}
	The integrative analysis of diverse data modalities in a systems biology approach will capture better the molecular phenotypic varaition of biological systems
	\centering
	\includegraphics[height=6cm]{img/Sun2016_multiomics.png}
	\end{frame}

	\begin{frame}{Strategies for multi-omics data integration}
	Choosing a common coordinate framework:
	\begin{itemize}
		\item Samples as the common coordinate framework: \textit{matched} data sets.
		\item Features as the common coordinate framework: \textit{unmatched} data sets.
		\item No common coordinate framework (in the high-dimensional space).
	\end{itemize}	
	\end{frame}

	\begin{frame}{Strategies for multi-omics data integration in matched assays}
	By having a common sample space, the aim of these methods is to discover and exploit associations within and between molecular layers. Broadly speaking, there are two types of integration strategies:
	\centering
	\includegraphics[height=6cm]{img/Ritchie2015_local_vs_global.png}
	\end{frame}

	\begin{frame}{Local analysis}
	\includegraphics[height=6cm]{img/Ritchie2015_local_analysis.png}
	\end{frame}

	\begin{frame}{Local analysis: linear models for genetic associations}
	\includegraphics[height=6cm]{img/gwas_eqtl.png}
	\end{frame}

	\begin{frame}{Global analysis}
	\includegraphics[height=6cm]{img/Ritchie2015_global_analysis.png}
	\end{frame}

	\begin{frame}{Challenges in (global) multi-omics data integration}
	\begin{itemize}
		\item Data collected using different techniques (i.e. data modalities) generally exhibit heterogeneous statistical properties
		\item Large amounts (and different patterns) of missing values
		\item Overfitting
		\item Undesired sources of heterogeneity
		\item Complexity of the data requires unsupervised interpretable approaches
	\end{itemize}
	\end{frame}

	\begin{frame}{Latent variable models}
	A key principle of biological data is that variation between the features is coordinated and results from the existence of underlying biological processes (for example gene regulatory networks). This key assumption sets off an entire statistical framework of exploiting the redundancy encoded in the data set to infer the latent sources of variation
	\end{frame}

	\begin{frame}{Latent variable models}
	Given a dataset $\mathbf{Y}$ of $N$ samples and $D$ features, latent variable models attempt to exploit the dependencies between the features by reducing the dimensionality of the data to a small set of $K$ latent variables. The mapping between the low-dimensional space and the high-dimensional space is performed via a function $f(\bfX|\bTheta)$
	\includegraphics[height=5cm]{img/latent_variable_models.png}
	\end{frame}

	\begin{frame}{Principal component analysis}
	Principal Component Analysis (PCA) is the most popular technique for dimensionality reduction \cite{Hotelling1933,Ringner2008} which defines $f(\bfX|\bTheta)$ to be a linear mapping. Mathematically, the transformation is defined by a matrix $\bfW \in \R^{D \times K}$ of feature weights that maps the observations $\bfY \in \R^{N \times D}$ to the latent space $\bfZ \in \R^{N \times K}$:
	\begin{equation}
		\mathbf{Z} = \mathbf{Y}\mathbf{W}
	\end{equation}
	% The matrix  contains the low-dimensional representation for each sample (i.e. the factors).\\
	% The matrix $\bfW \in \R^{D \times K}$ contains the feature weights (i.e. the gene scores)
	\end{frame}

	\begin{frame}{Mathematical derivation of PCA: maximum variance formulation}
	In the maximum variance formulation, the aim is to infer an orthonormal matrix $\bfW$ such that the variance of $\bfZ$ (the projected data) is maximised.\\
	If we consider a single principal component, the variance $\sigma$ of the projected data is:
	\begin{equation}
		\sigma = \frac{1}{N}\sum_{n=1}^{N} (\bfz_n - \hat{\bfz})^{2} = \frac{1}{N}\sum_{n=1}^{N} (\bfy_n \bfw - \hat{\bfy}\bfw)^{2}
	\end{equation}
	where $\hat{\bfy}$ is a vector with the feature-wise sample means.\\
	This can be written in a more compact matrix form by defining $\bfS \in \R^{D \times D}$ as the data covariance matrix: $\bfS = \frac{1}{N}\sum_{n=1}^{N} (\bfy_n - \hat{\bfy})(\bfy_n - \hat{\bfy})^T$: %Note that if the data is centered this is equal to $\bfS = \bfY \bfY^T. Thus:
	\begin{equation}
		\sigma = \frac{1}{N}\sum_{n=1}^{N} (\bfw \bfS \bfw)
	\end{equation}
	\textbf{(Q) Maximising this expression does not work, we need a constraint. Why?}
	% (Solution) The weights need to be constraint to have a constant norm (one, arbritrarily), otherwise maximizing the expression above would simply lead to the weight vector having infinite norm, as this leads to an infinite variance.
	\end{frame}

	% \begin{frame}{Mathematical derivation of PCA: maximum variance formulation}
	% The optimisation problem to find the first principle component can be defined as:
	% \begin{equation}
	% 	\bfw^T \bfS \bfw
	% \end{equation}\\
	% \end{frame}

	\begin{frame}{Mathematical derivation of PCA: maximum variance formulation}
	The constrained optimisation problem can be defined as:
	\begin{equation}
		\argmax_{\|\bfw\|=1} = \bfw^T \bfS \bfw
	\end{equation}
	It can be solved by introducing a Lagrange multiplier $\lambda$ to enforce the constraint:
	\begin{equation}
		\bfw^T \bfS \bfw + \lambda (1-\bfw^T\bfw)
	\end{equation}
	By setting the derivative w.r.t. $\bfw$ to zero, we obtain the following equation:
	\begin{equation}
		\bfS \bfw = \lambda \bfw
	\end{equation}
	which should be familiar (perhaps in this form $\bfA \bfv = \lambda \bfv$)!
	\end{frame}

	\begin{frame}{Finding subsequent principal components}
	$\bfw$ must be an eigenvector of $\bfS$, which is known as the first principal component. Simple algebra shows that $\lambda$ corresponds to the variance $\sigma$
	\end{frame}

	\begin{frame}{Finding subsequent principal components}
	The $k$-th principal component can be found by subtracting from $\bfY$ the reconstructed data by the previous $k-1$ principal components and repeating the procedure above. If we define $\bfz_k=\bfw_k^T \bfY$ to be the $k$-th principal component:
	\begin{equation}
		\hat{\bfY} = \bfY - \sum_{k=1}^{K} (\bfz_k \bfw_k^T)
	\end{equation}
	In conclusion, among all possible orthonormal basis of dimensionality $K$, the one that maximises the projected variance corresponds to the basis defined by the first $K$ eigenvectors of the covariance matrix $\bfS$.

	 % all the axes can be found in one operation called the Singular Value Decompositio
	


	%http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/

	\end{frame}

	% \begin{frame}{Mathematical derivation of PCA: minimum error formulation}
	% In the minimum error formulation, the aim is to infer an orthonormal projection of the data onto a low-dimensional space such that it minimises the mean squared error between the observations and the reconstructed data using $K$ latent variables (i.e. principal components):
	% \begin{equation}
	% 	\argmax_{\|\bfw\|=1} \Vert \bfY - \sum_{k=1}^{K} (\bfz_k \bfw_k^T) \Vert^{2}
	% \end{equation}
	% where $\Vert \cdot \Vert^{2}$ is the Frobenius norm.\\
	% Using $\bfz_k$ The eqiation can be rewritten as:
	% \begin{equation}
	% 	\argmax_{\|\bfw\|=1} \Vert \bfY - \sum_{k=1}^{K} (\bfz_k \bfw_k^T) \Vert^{2}
	% \end{equation}


	% The optimisation problem can be solved by introducing a Lagrange multiplier $\lambda$ to enforce the constraint:
	% \begin{equation}
	% 	\Vert \bfY - \sum_{k=1}^{K} (\bfz_k \bfw_k^T) \Vert^{2} + \lambda (1-)
	% \end{equation}
	% \end{frame}

	% In both cases, solving the optimisation problems via Lagrange multipliers leads, remarkably, to the same solution:
	% \begin{equation}
	% 	\bfS \bfw_k = \lambda_k \bfw_k
	% \end{equation}
	% Hence, the weight vectors $\bfw_k$ are the eigenvectors of $\bfS$, which can be computed via singular value decomposition \cite{Bishop2006}.

	% \begin{frame}{Why do the two PCA formulations lead to the same answer?}
	% The reason why the maximum variance solution and the minimum reconstruction error solution are the same can be understood by applying Pythagoras theorem to the right triangle defined by the projection of a sample $\bfy_{n}$ to a weight vector $\bfw$.\\
	% Assuming again centered data, the variance of $\bfy_{n}$ is $\|\bfy_{n}\| = \bfy_{n}^T \bfy_{n}$. This variance decomposes as the sum of the variance in the latent space $\|\bfz_{n}\| = \bfz_{n}^T \bfz_{n}$ and the residual variance after reconstruction $\|\bfy_{n} - \bfz_{n} \bfw^T \|$:
	% \includegraphics[height=5cm]{img/pca2}
	% \end{frame}

	% \begin{frame}{Principal component analysis}
	% \centering
	% \includegraphics[height=7cm]{img/pca.jpeg}\\
 %  	\tiny Credit to Raunak Joshi \par
	% \end{frame}
	
	\begin{frame}{Problems of using PCA for multi-omics data integration}
	\begin{itemize}
		\item Does not generalise to an arbitrary number of data modalities.
		\item No natural way to combine different data modalities (binary data with continuous data).
		\item Cannot handle missing values.
		\item Non-sparse loadings: challenges in interpretability and risk of overfitting.
	\end{itemize}
	\end{frame}

	\begin{frame}{Factor analysis}
	PCA is a purely algebraic method, with no assumptions regarding the data generation processes and no uncertainity quantification. It is just a technique to summarise the data. Factor analysis is a probabilistic generalisation of PCA. It assumes the following underlying generative model:
	\begin{equation}
		\bfY = \bfW \bfZ + \bepsilon
	\end{equation}
	where the weights $\bfW$ are assumed to be non-probabilistic parameters, but the noise $\bepsilon$ and the latent variables $\bfZ$ (the principal components) are assumed to follow an isotropic normal prior distribution:
	\begin{align*}
		p(\bfZ) &= \prod_{n=1}^{N} \prod_{k=1}^{K} \Ndist{z_{nk}}{0,1} \\
		p(\epsilon) &= \prod_{d=1}^{D} \Ndist {\epsilon_d}{0,\sigma_{d}^{2}}
	\end{align*}
	which leads to a normally-distributed likelihood:
	\begin{equation}
		p(\bfY|\bfW,\bfZ,\sigma) = \prod_{n=1}^{N} \prod_{d=1}^{D} \Ndist{y_{n,d}}{\bfw_{k}^T \bfz_{n},\sigma^2 \I}
	\end{equation}
	\end{frame}

	\begin{frame}{Graphical model for factor analysis}
	\includegraphics[height=7cm]{img/graphical_model_pca.png}
	\end{frame}


	\begin{frame}{Canonical correlation analysis}

	Canonical Correlation Analysis (CCA) is a simple extension of PCA to find linear components that capture correlations between two datasets \cite{Hotteling1936,Hardle2007}.\\
	Given two data matrices $\bfY_1 \in \R^{N \times D_1}$ and $\bfY_2 \in \R^{N \times D_2}$ CCA finds a set of linear combinations $\bfU \in \R^{D_1 \times K}$ and $\bfV \in \R^{D_2 \times K}$ with maximal cross-correlation.\\ For the first pair of canonical variables, the optimisation problem is:
	\begin{equation}
		(\hat{\bfu_1}, \hat{\bfv_1}) = \argmax_{\bfu_1,\bfv_1} corr(\bfu_{1}^T \bfY_1, \bfv_{1}^T \bfY_2)
	\end{equation}
	As in conventional PCA, the linear components are constraint to be orthonormal. Hence, the first pair of canonical variables $\bfu_1$ and $\bfv_1$ contain the linear combination of variables that have maximal correlation. Subsequently, Therefore, the second pair of canonical variables $\bfu_2$ and $\bfv_2$ is found out of the residuals of the first canonical variables.\\
	(Q) What are the limitation of CCA?
	\end{frame}

	\begin{frame}{Probabilistic canonical correlation analysis}
	CCA can also be formulated as a probabilistic generative model \cite{Bach2005}. The two matrix of observations $\bfY^{1}$ and $\bfY^{2}$ are decomposed in terms of two weight matrices $\bfW^{1}$ and $\bfW^{2}$ but a joint latent matrix $\bfZ$:
	\begin{equation}
		\bfY^{m} = \bfW^{m} \bfZ + \epsilon^{m}
	\end{equation}
	where $m$ is the index for the views. We specify the following probability distributions for the factors and the noise:
	\begin{align*}
		p(z_{nk}) = \Ndist{z_{nk}}{0,1} \qquad
		p(\epsilon^{m}) = \Ndist {\epsilon^{m}}{\tau_{m}^{-1}}
	\end{align*}
	The weights and the variance of the noise are assumed to be non-probabilistic parameters. This yields the following likelihood functions:
	\begin{equation}
		p(\bfY|\bfW,\bfZ,\tau) = \prod_{m=1}^{M} \prod_{n=1}^{N} \prod_{d=1}^{D_1} \Ndist{y^{m}_{n,d}}{(\bfw_{:,k}^{m})^T \bfz_{n},\tau_{m}^{-1}}
	\end{equation}
	\end{frame}

	\begin{frame}{Graphical model for canonical correlation analysis}
	\centering
	\includegraphics[height=6cm]{img/graphical_model_cca.png}
	\end{frame}


	% \begin{frame}

	% No, this is
	
	% \end{frame}
	\begin{frame}{References}
  	\printbibliography
	\end{frame}

\end{document}
