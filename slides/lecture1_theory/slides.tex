\documentclass[aspectratio=169,notes]{beamer}

\usepackage{pgfpages}

% Allows including images
\usepackage{graphicx} 

% ???
\usepackage[english]{babel}

%\setbeameroption{show notes on second screen}
%\setbeameroption{hide notes}

% Use metropolis theme
\usetheme[
	outer/progressbar=foot,
	outer/numbering=counter,
	%titleformat=allsmallcaps
	]{metropolis} 
\definecolor{blue}{HTML}{056466}
\definecolor{white}{HTML}{FFFFFF}

 % Sans Serif Font
%\usepackage[semibold,oldstyle]{sourcesanspro}

 % The command \nth{<number>} generates English ordinal numbers of the form 1st, 2nd, 3rd, 4th
%\usepackage[super]{nth}

% Theme colors are derived from these two elements
\setbeamercolor{alerted text}{fg=blue}
\setbeamercolor{frametitle}{bg=blue}
\setbeamercolor{normal text}{bg=white}

% Set EBI logo footer
%  \setbeamertemplate{footline}{%
%         \includegraphics[height=1cm]{img/EBI_logo.png}%
% 		%\hfill%
% 		% \insertsectionnavigationhorizontal{.5\textwidth}{}{}
% }


\title{Statistical methods for data integration}
\author{Ricard Argelaguet \\ \textit{ricard@ebi.ac.uk}}
\institute{European Bioinformatics Institute (EMBL-EBI) \\ University of Cambridge}
%\date{\today}


%  (SI) units 
\usepackage{siunitx}

% bibliography
% \usepackage[style=authoryear, backend=biber, sortlocale=en_US, natbib=false, url=false, doi=true, eprint=false]{biblatex}
% \usepackage[style=authoryear,,backend=bibtex]{biblatex}
\usepackage[hyperref=true, url=false, backref=false, backend=biber, style=authoryear, giveninits=true]{biblatex}
\addbibresource{references/bibliography.bib}
\renewcommand*{\bibfont}{\tiny}

% \renewbibmacro*{cite}{
%   \iffieldundef{shorthand}
%     {\ifthenelse{\ifnameundef{labelname}\OR\iffieldundef{labelyear}}
%        {\usebibmacro{cite:label}
%         \setunit{\printdelim{nonameyeardelim}}}
%        {\printnames{labelname}
%         \setunit{\printdelim{nameyeardelim}}}
%      \usebibmacro{cite:labeldate+extradate}
%      \setunit{\addcomma\space}
%      \usebibmacro{journal}}
%     {\usebibmacro{cite:shorthand}}
% }

\DeclareCiteCommand{\footcite}[\mkbibfootnote]
  {\usebibmacro{prenote}}
  {\printnames{author}
   \printfield{title}{}
   \printfield{journaltitle}{}
   \printfield{year}}
  {\addsemicolon\space}
 {\usebibmacro{postnote}}
  \newcommand\footnotenonum[1]{
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}
  \addtocounter{footnote}{-1}
  \endgroup
}

%\setbeamerfont{footnote}{size=\footnotesize}
\setbeamerfont{footnote}{size=\tiny}

% Foot note without markers
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

% Remove indent in the footnotes
\addtobeamertemplate{footnote}{\hskip -1em}{}


\input{utils.tex}


\begin{document}

 	% Print the title page as the first slide
	\begin{frame}
	\titlepage
	\end{frame}
	
	% \begin{frame}{Overview}
	% \end{frame}

	% \begin{frame}{Why multi-omics?}
	% \centering
	% \includegraphics[height=6cm]{img/Ritchie2015_overview.png}\footcite{Ritchie2015}
	% \end{frame}
	
	\begin{frame}{Why multi-omics?}
	The integrative analysis of diverse data modalities in a systems biology approach will capture better the molecular phenotypic varaition of biological systems\\
	\leavevmode\newline
	\centering
	\includegraphics[height=5.5cm]{img/Sun2016_multiomics.png}
	\end{frame}

	\begin{frame}{Strategies for multi-omics data integration}
	The first step is to choose the common coordinate framework for the integration:
	\begin{itemize}
		\item \textbf{Samples} as the common coordinate framework (\textit{matched} data sets).
		\item \textbf{Features} as the common coordinate framework (\textit{unmatched} data sets).
	\end{itemize}	
	\end{frame}

	% \begin{frame}{Strategies for multi-omics data integration in \textit{matched} assays}
	% By having a common sample space, the aim of these methods is to discover and exploit associations within and between molecular layers. Broadly speaking, there are two types of integration strategies:
	% \centering
	% \includegraphics[height=6cm]{img/Ritchie2015_local_vs_global.png}
	% \end{frame}

	\begin{frame}{Strategies for multi-omics data integration}
	Two general strategies for the analysis of \textit{matched} assays:
	\begin{itemize}
		\item \textbf{Local analysis}: test for marginal associations between features from different molecular layers. Generally supervised.
		\item \textbf{Global analysis}: exploit the dependencies between the features to construct a mathematical representation of the data. Generally unsupervised.
	\end{itemize}	
	\end{frame}

	\begin{frame}{Local analysis}
	The most prominent examples of local analysis are quantitative trait loci mapping (GWAS and eQTLs)\footcite{Ritchie2015}:\\
	\leavevmode\newline
	\centering
	\includegraphics[height=5cm]{img/Ritchie2015_local_analysis.png}
	\blfootnote{eQTL: expression Quantiative Trait Loci}
	\end{frame}

	\begin{frame}{Local analysis}
	Local analysis is typically done using (generalised) linear models\\
	\leavevmode\newline
	\centering
	\includegraphics[height=5.5cm]{img/gwas_eqtl.png}
	\blfootnote{GWAS: Genome-Wide Association Study (GWAS)\\eQTL: expression Quantiative Trait Loci\\Slide by Anna Cuomo}
	\end{frame}

	\begin{frame}{Local analysis}
	\leavevmode\newline
	\centering
	\includegraphics[height=6cm]{img/linear-regression.png}
	\end{frame}



	\begin{frame}{Global analysis}
	In global analysis the aim is to exploit the relationship between all features to create useful mathematical representations\footcite{Ritchie2015}\\
	\leavevmode\newline
	\centering
	\includegraphics[height=4.5cm]{img/Ritchie2015_global_analysis.png}
	\end{frame}


	\begin{frame}{Global analysis}
	\leavevmode\newline
	\centering
	\includegraphics[height=6cm]{img/george_box_model.png}
	\end{frame}

	\begin{frame}{Global analysis}
	Challenges in (global) multi-omics data integration:
	\begin{itemize}
		\item Data collected using different techniques (i.e. data modalities) generally exhibit heterogeneous statistical properties
		\item Large amounts (and different patterns) of missing values
		\item Overfitting
		\item Undesired sources of heterogeneity
		\item Complexity of the data requires unsupervised interpretable approaches
	\end{itemize}
	\end{frame}

	% \begin{frame}{Latent variable models}
	% A key principle of biological data is that variation between the features is coordinated and results from the existence of underlying biological processes (for example gene regulatory networks). This key assumption sets off an entire statistical framework of exploiting the redundancy encoded in the data set to infer a meaningful low-dimensional repesentation of the data that captures as much signal as possible from the input data.
	% % Latent variable models (LVMs) are arguably the most successful approaches for biological data analysis.
	% \end{frame}

	\begin{frame}{Latent variable models}
	Given a dataset $\mathbf{Y}$ of $N$ samples and $D$ features, latent variable models exploit the dependencies between the features to reduce the dimensionality of the data to a small set of $K$ latent variables. The mapping from the high-dimensional to the low-dimensional space is performed via a function $f(\bfY|\bTheta)$:\\
	\leavevmode\newline
	\centering
	\includegraphics[height=4cm]{img/latent_variable_models.png}
	\end{frame}

	\begin{frame}{Principal component analysis (PCA)}
	Principal Component Analysis (PCA) is the most popular technique for dimensionality reduction.\\
	\leavevmode\newline
	\centering
	\includegraphics[height=4.75cm]{img/pca.jpeg}\\
  	\tiny Credit to Raunak Joshi \par
	\end{frame}


	\begin{frame}{Principal component analysis (PCA)}
	PCA defines $f(\bfY|\bTheta)$ to be a linear transformation via a matrix $\bfW \in \R^{D \times K}$ that maps the observations $\bfY \in \R^{N \times D}$ onto the latent space $\bfZ \in \R^{N \times K}$.\\
	\leavevmode\newline
	\centering
	\includegraphics[height=3.5cm]{img/matrix_factorisation.png}
	% \centering
	% \includegraphics[height=2.8cm]{img/pca_overview.pdf}
	% \[
	% 	\mathbf{Z} = \mathbf{Y}\mathbf{W}
	% \]
	% The matrix  contains the low-dimensional representation for each sample (i.e. the factors).\\
	% The matrix $\bfW \in \R^{D \times K}$ contains the feature weights (i.e. the gene scores)
	\end{frame}

	% \begin{frame}{Principal component analysis (PCA)}
	% \end{frame}

	\begin{frame}{Mathematical derivation of PCA: maximum variance formulation}
	The aim in PCA is to infer the matrix $\bfW$ such that the variance of $\bfZ$ (the projected data) is maximised. If we consider a single latent factor, the variance of the projected data is:
	\begin{align*}
		\sigma^2 &= \frac{1}{N}\sum_{n=1}^{N} (\bfz_n - \hat{\bfz})^{2} \\
				 &= \frac{1}{N}\sum_{n=1}^{N} (\bfy_n^T \bfw - \hat{\bfy}^T\bfw)^{2}
	\end{align*}
	where $\hat{\bfy}$ is a vector with the feature-wise means. If we center the data this simplifies to:
	\begin{equation*}
		\sigma^2 = \frac{1}{N}\sum_{n=1}^{N} (\bfy_n^T \bfw)^{2}
	\end{equation*}
	\end{frame}

	\begin{frame}{Mathematical derivation of PCA: maximum variance formulation}
	A bit of algebra allows us to define this equation in terms of the (centered) data covariance matrix: $\bfS = \frac{1}{N}\sum_{n=1}^{N} \bfy_n\bfy_n^T$:
	\begin{align*}
		\sigma^2 &= \frac{1}{N}\sum_{n=1}^{N} (\bfy_n^T \bfw)^T (\bfy_n^T \bfw) \\
		=& (\bfw^T \bfy_n) (\bfy_n^T \bfw) \\
		=& \bfw^T (\bfy_n \bfy_n^T) \bfw \\
		=& \bfw^T \bfS \bfw
	\end{align*}
	\end{frame}

	\begin{frame}{Mathematical derivation of PCA: maximum variance formulation}
	The optimisation problem to find the first latent variable could be defined as:
	\[
		\hat{\bfw} = \argmax_{\bfw} \bfw^T \bfS \bfw
	\]
	% where we have removed any term that is constant and does not affect the optimisation.\\
	\leavevmode\newline
	\textbf{(Q) Maximising this expression does not work, we need a constrain. Why?}
	% (Solution) The weights need to be constraint to have a constant norm (one, arbritrarily), otherwise maximizing the expression above would simply lead to the weight vector having infinite norm, as this leads to an infinite variance.
	\end{frame}

	\begin{frame}{Mathematical derivation of PCA: maximum variance formulation}
	The constrained optimisation problem can be defined as:
	\[
		\hat{\bfw} = \argmax_{\|\bfw\|=1} \bfw^T \bfS \bfw
	\]
	It can be solved by introducing a Lagrange multiplier $\lambda$ to enforce the constraint:
	\[
		f(\bfW,\lambda) = \bfw^T \bfS \bfw + \lambda (1-\bfw^T\bfw)
	\]
	By setting the derivative $\frac{\partial f(\bfW,\lambda)}{\partial \bfw}$ to zero, we obtain the following equation:
	\[
		\bfS \bfw = \lambda \bfw
	\]
	which should be familiar (perhaps in this form $\bfA \bfv = \lambda \bfv$)?
	\end{frame}

	\begin{frame}{Mathematical derivation of PCA: maximum variance formulation}
	Among all possible orthonormal basis, the one that maximises the projected variance corresponds to the basis defined by the eigenvectors of the covariance matrix $\bfS$. These vector basis are called the principal components.\\
	\leavevmode\newline
	The corresponding eigenvalue $\lambda$ corresponds to the variance $\sigma$ (proof in the appendix).
	% \leavevmode\newline
	% To obtain the low-dimensional representation of the data we just need to multiply each data point $\bfy_n$ by $\bfw$:
	% \[
	% 	z_n = \bfy_n^T \bfw 
	% \]
	\end{frame}

	% \begin{frame}{Mathematical derivation of PCA: maximum variance formulation}
	% The eigenvalue $\lambda$ corresponds to the variance $\sigma$. The proof requires knowing the following equality $Var(X) = \E[X^2] - (\E[X])^2$

	% some algebra:
	% \begin{align}
	% \sigma^2 = Var($\bfz$) &= \E[\bfz^T\bfz] - (\E[\bfz])^2 \\
	% &= \E[(\bfY\bfw)^T (\bfY\bfw)] - (\E[\bfY \bfw])^2 \\
	% % &= \E[(\bfy^T\bfw)(\bfy^T\bfw)] - (\bfw \E[\bfy^T])^2 \\
	% \end{align}
	% If we assume that the data is centered then $\E[\bfy^T]=0$, so the second term disappears.
	% \[
	% \sigma^2 = \E[\bfw \bfY^T \bfT \bfw]
	% \]
	% where the term $\bfY^T \bfY$ corresponds to the covariance matrix $\bfS$. Therefore:
	% \[
	% \sigma^2 = \E[\bfw \bfS \bfw] = 
	% \]
	% \end{frame}

	% \sigma^2 = \frac{1}{N}\sum_{n=1}^{N} (\bfz_n - \hat{\bfz})^{2} = \frac{1}{N}\sum_{n=1}^{N} (\bfy_n \bfw - \hat{\bfy}\bfw)^{2}


	\begin{frame}{Generalisation to multiple principal components}
	Most data can not be well-described by a single principal component. The $k$-th principal component can be found by subtracting from $\bfY$ the reconstructed data by the previous $k-1$ principal components: 
	%If we define $\bfz_k=\bfY \bfw_k $ to be the $k$-th principal component:
	\[
		\hat{\bfY} = \bfY - \sum_{k=1}^{K} (\bfz_k \bfw_k^T)
	\]
	and repeating the procedure above using the reconstructed covariance matrix $\hat{\bfS}$ as input
	% \[
	% 	\hat{\bfw_k} = \argmax_{\|\bfw_k\|=1} \bfw_{k}^{T} \hat{\bfS_k} \bfw_k
	% \]
	% \centering
	% \includegraphics[height=3cm]{img/pca_reconstruction.png}
	% \blfootnote{Figure by Alex Williams} %http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/
	\end{frame}

	\begin{frame}{Finding the \textit{right} number of Principal Components}
	Principal components are ranked by the amount of variance they capture in the original dataset, a scree plot can provide some sense of how many components are needed.\\
	\leavevmode\newline
	\centering
	\includegraphics[height=4cm]{img/scree_plot.png}
	\blfootnote{Figure by Alex Williams} %http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/
	\end{frame}

	% \begin{frame}{Why is the weight/loading matrix $\bfW$ called the rotation matrix?}
	% The eigendecomposition of the covariance matrix has two components scaling (defined by the eigenvalues) and rotation (defined by the eigenvectors). The eigenvectors are orthonormal matrices that perform a rotation on the high-dimensional space to align (i.e. rotate!) the data onto the principal components. They do not perform any scaling per se, and hence this is equivalent to a rotation matrix.
	% \blfootnote{https://towardsdatascience.com/visualizing-principal-component-analysis-with-matrix-transforms-d17dabc8230e}
	% \end{frame}


	% \begin{frame}{Mathematical derivation of PCA: minimum error formulation}
	% In the minimum error formulation, the aim is to infer an orthonormal projection of the data onto a low-dimensional space such that it minimises the mean squared error between the observations and the reconstructed data using $K$ latent variables (i.e. principal components):
	% \[
	% 	\argmax_{\|\bfw\|=1} \Vert \bfY - \sum_{k=1}^{K} (\bfz_k \bfw_k^T) \Vert^{2}
	% \]
	% where $\Vert \cdot \Vert^{2}$ is the Frobenius norm.\\
	% Using $\bfz_k$ The eqiation can be rewritten as:
	% \[
	% 	\argmax_{\|\bfw\|=1} \Vert \bfY - \sum_{k=1}^{K} (\bfz_k \bfw_k^T) \Vert^{2}
	% \]

	% The optimisation problem can be solved by introducing a Lagrange multiplier $\lambda$ to enforce the constraint:
	% \[
	% 	\Vert \bfY - \sum_{k=1}^{K} (\bfz_k \bfw_k^T) \Vert^{2} + \lambda (1-)
	% \]
	% \end{frame}

	% In both cases, solving the optimisation problems via Lagrange multipliers leads, remarkably, to the same solution:
	% \[
	% 	\bfS \bfw_k = \lambda_k \bfw_k
	% \]
	% Hence, the weight vectors $\bfw_k$ are the eigenvectors of $\bfS$, which can be computed via singular value decomposition \cite{Bishop2006}.

	% \begin{frame}{Why do the two PCA formulations lead to the same answer?}
	% The reason why the maximum variance solution and the minimum reconstruction error solution are the same can be understood by applying Pythagoras theorem to the right triangle defined by the projection of a sample $\bfy_{n}$ to a weight vector $\bfw$.\\
	% Assuming again centered data, the variance of $\bfy_{n}$ is $\|\bfy_{n}\| = \bfy_{n}^T \bfy_{n}$. This variance decomposes as the sum of the variance in the latent space $\|\bfz_{n}\| = \bfz_{n}^T \bfz_{n}$ and the residual variance after reconstruction $\|\bfy_{n} - \bfz_{n} \bfw^T \|$:
	% \includegraphics[height=5cm]{img/pca2}
	% \end{frame}

	\begin{frame}{Problems of using PCA for multi-omics data integration}
	PCA is a great exploratory tool for single multivariate data sets, but it has important pitfalls in the analysis of multi-omics data:
	\begin{itemize}
		\item Does not generalise to an arbitrary number of data modalities.
		\item No natural way to combine different data modalities (binary data with continuous data).
		\item Cannot handle missing values.
		% \item Non-sparse loadings: challenges in interpretability and risk of overfitting.
	\end{itemize}
	\end{frame}

	\begin{frame}{Probabilistic PCA and Factor analysis}
	PCA is a purely algebraic method to summarise the data. It makes no assumptions regarding the data generation processes and no uncertainity quantification.\\
	Factor analysis is a probabilistic generalisation of PCA:
	\[
		\bfY = \bfW \bfZ + \bepsilon
	\]
	where the weights $\bfW \in \R^{D \times K}$ are assumed to be non-probabilistic parameters, but the noise $\epsilon \in \R^{1 \times 1}$ and the latent variables $\bfZ \in \R^{N \times K}$ are normally-distributed random variables:
	\begin{align*}
		p(\bfZ) &= \prod_{n=1}^{N} \prod_{k=1}^{K} \Ndist{z_{nk}}{0,1} \\
		\\
		p(\epsilon) &= \Ndist {\epsilon}{0,\sigma^{2}}
	\end{align*}
	\end{frame}

	\begin{frame}{Factor analysis}
	All together, this leads to a normal distributed for the observations:
	\[
		p(\bfY|\bfW,\bfZ,\sigma) = \prod_{n=1}^{N} \prod_{d=1}^{D} \Ndist{y_{n,d}}{\bfw_{k}^T \bfz_{n},\sigma^2}
	\]
	This is called the likelihood and it corresponds to the objective function that needs to be maximised when training the model.
	\end{frame}

	\begin{frame}{Graphical model for factor analysis}
	\centering
	\includegraphics[height=6.5cm]{img/graphical_model_pca.png}
	\blfootnote{N: number of samples\\D: number of features\\K: number of latent variables}
	\end{frame}


	\begin{frame}{Canonical correlation analysis}
	Canonical Correlation Analysis (CCA) is a simple extension of PCA to find linear components that capture correlations between \textbf{two} datasets \footcite{Hotteling1936}.\\
	\leavevmode\newline
	Given two data matrices $\bfY_1 \in \R^{N \times D_1}$ and $\bfY_2 \in \R^{N \times D_2}$ CCA finds a set of linear combinations $\bfU \in \R^{D_1 \times K}$ and $\bfV \in \R^{D_2 \times K}$ with maximal cross-correlation.\\ 
	\leavevmode\newline
	For the first pair of canonical variables, the optimisation problem is:
	\[
		(\hat{\bfu_1}, \hat{\bfv_1}) = \argmax_{\|\bfu_1\|=1,\|\bfv_1\|=1} corr(\bfu_{1}^T \bfY_1, \bfv_{1}^T \bfY_2)
	\]
	%As in PCA, the linear components are constraint to be orthonormal. 
	%Hence, the first pair of canonical variables $\bfu_1$ and $\bfv_1$ contain the linear combination of variables that have maximal correlation. Subsequently, Therefore, the second pair of canonical variables $\bfu_2$ and $\bfv_2$ is found out of the residuals of the first canonical variables.\\
	% (Q) What are the limitation of CCA?
	\end{frame}

	\begin{frame}{Probabilistic canonical correlation analysis}
	CCA can also be formulated as a probabilistic generative model \footcite{Bach2005}. The two matrix of observations $\bfY^{1}$ and $\bfY^{2}$ are decomposed in terms of two weight matrices $\bfW^{1}$ and $\bfW^{2}$ but a joint latent matrix $\bfZ$:
	\begin{align*}
		\bfY^{1} = \bfW^{1} \bfZ + \epsilon^{1} \qquad 
		\bfY^{2} = \bfW^{2} \bfZ + \epsilon^{2}
	\end{align*}
	The weights are assumed to be non-probabilistic parameters. We specify the following probability distributions for the latent variables:
	\begin{align*}
		p(z_{nk}) &= \Ndist{z_{nk}}{0,1}
	\end{align*}
	The noise for each data modality is also defined as a normal distribution:
	\begin{align*}
		p(\epsilon^{1}) = \Ndist{\epsilon^{1}}{0,\sigma_{1}^2} \qquad 
		p(\epsilon^{2}) = \Ndist{\epsilon^{2}}{0,\sigma_{2}^2}
	\end{align*}
	\end{frame}

	\begin{frame}
	All together, this yields the following likelihood function:
	\[
		p(\bfY|\bfW,\bfZ,\tau) = \prod_{m=1}^{M=2} \prod_{n=1}^{N} \prod_{d=1}^{D_1} \Ndist{y^{m}_{n,d}}{(\bfw_{k}^{m})^T \bfz_{n},\sigma_{m}^2}
	\]
	This will correspond to the objective function that needs to be maximised when training the model.
	\end{frame}

	\begin{frame}{Graphical model for canonical correlation analysis}
	\centering
	\includegraphics[height=6cm]{img/graphical_model_cca.png}
	\end{frame}


	% \begin{frame}

	% No, this is
	
	% \end{frame}
	\begin{frame}[allowframebreaks]{References}
  	\printbibliography[heading=none]
	\end{frame}

\end{document}
